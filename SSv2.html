
<!DOCTYPE HTML>

<html>

<head>
  <title> SSv2</title>

  <link rel="stylesheet" href="test.css">
</head>


<body>
  <br>

  <center><span style="font-size: 44px; font-weight: bold;"> Something-Something V2 Dataset</span></center><br/>
<br>

<div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
The Something-Something V2 dataset currently contains 108, 499 videos across 174 labels, with duration ranging from 2 to 6 seconds. Labels are
textual descriptions based on templates, such as “Dropping
[something] into [something]” containing slots (“[something]”) that serve as placeholders for objects. Crowdworkers provide videos where they act out the templates.
They choose the objects to perform the actions on and enter
the noun-phrase describing the objects when uploading the
videos.
The dataset is split into train, validation and test-sets in
the ratio of 8:1:1. The splits were created so as to ensure
that all videos provided by the same worker occur only in
one split (train, validation, or test).In its current version, the dataset was generated by 1133
crowd workers with an average of 127.32 workers per class.
<br>
<br>
More details can be found  <a href="https://developer.qualcomm.com/software/ai-datasets/something-something"> here. </a>
</div>
<br>
<hr> <br>
<center>
<h2> Sample Frames </h2>

<center>
  <img src="./ssv2_frames.JPG" width=900px/><br>

  </center>
  <br>
  <hr> <br>
</body>

</html>
