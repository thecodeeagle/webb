
<!DOCTYPE HTML>

<html>

<head>
  <title> VideoSwin </title>

  <link rel="stylesheet" href="test.css">
</head>


<body>
  <br>

  <center><span style="font-size: 44px; font-weight: bold;"> VideoSwin </span></center><br/>
<br>
<div style="width: 750px; margin: 0 auto; text-align=center; text-align: justify; text-justify: inter-ideograph;">
  VideoSwin is a pure-transformer backbone architecture for video recognition that is found
to surpass the factorized models in efficiency. It achieves this by taking advantage of the inherent spatiotemporal locality of videos, in which pixels that are closer to each other in spatiotemporal
distance are more likely to be correlated. Because of this property, full spatiotemporal self-attention
can be well-approximated by self-attention computed locally, at a significant saving in computation
and model size. The approach is implemented through an adaptation of the Swin Transformer.


<br>
<br>
More details can be found  <a href="https://arxiv.org/abs/2106.13230"> here. </a>
<br>
<br>
<center>
  <img src="./videoswin_arch.JPG"  ><br>
 Overall architecture of Video Swin Transformer (tiny version, referred to as Swin-T).

  </center>
  <br>
  <br>
</div>
</body>
</html>
